{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "```json\n{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import os\\n\",\n    \"import json\\n\",\n    \"import pandas as pd\\n\",\n    \"import numpy as np\\n\",\n    \"import torch\\n\",\n    \"from datasets import Dataset, DatasetDict, load_metric\\n\",\n    \"from transformers import (\\n\",\n    \"    AutoTokenizer, \\n\",\n    \"    AutoModelForSeq2SeqLM, \\n\",\n    \"    Seq2SeqTrainingArguments, \\n\",\n    \"    Seq2SeqTrainer, \\n\",\n    \"    DataCollatorForSeq2Seq,\\n\",\n    \"    pipeline\\n\",\n    \")\\n\",\n    \"import nltk\\n\",\n    \"import spacy\\n\",\n    \"from sklearn.model_selection import train_test_split\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"import seaborn as sns\\n\",\n    \"import optuna\\n\",\n    \"from pathlib import Path\\n\",\n    \"\\n\",\n    \"# Download necessary NLTK data\\n\",\n    \"try:\\n\",\n    \"    nltk.data.find('tokenizers/punkt')\\n\",\n    \"except nltk.downloader.DownloadError:\\n\",\n    \"    nltk.download('punkt')\\n\",\n    \"\\n\",\n    \"# Load spaCy model (download if needed)\\n\",\n    \"try:\\n\",\n    \"    nlp_spacy = spacy.load(\\\"en_core_web_sm\\\")\\n\",\n    \"except OSError:\\n\",\n    \"    print(\\\"Downloading spaCy en_core_web_sm model...\\\")\\n\",\n    \"    spacy.cli.download(\\\"en_core_web_sm\\\")\\n\",\n    \"    nlp_spacy = spacy.load(\\\"en_core_web_sm\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Configuration ---\\n\",\n    \"MODEL_NAME = \\\"facebook/bart-large-cnn\\\" # Pre-trained summarization model\\n\",\n    \"TOKENIZER_NAME = \\\"facebook/bart-large-cnn\\\"\\n\",\n    \"OUTPUT_DIR = \\\"./contextual_convo_condenser_model\\\"\\n\",\n    \"LOGGING_DIR = f\\\"{OUTPUT_DIR}/logs\\\"\\n\",\n    \"TRAIN_BATCH_SIZE = 4\\n\",\n    \"EVAL_BATCH_SIZE = 4\\n\",\n    \"NUM_TRAIN_EPOCHS = 3\\n\",\n    \"LEARNING_RATE = 5e-5\\n\",\n    \"WEIGHT_DECAY = 0.01\\n\",\n    \"MAX_INPUT_LENGTH = 1024\\n\",\n    \"MAX_TARGET_LENGTH = 128\\n\",\n    \"SEED = 42\\n\",\n    \"DEVICE = \\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\"\\n\",\n    \"USE_DUMMY_DATA = True # Set to False to load real data\\n\",\n    \"DUMMY_DATA_SIZE = 100\\n\",\n    \"TEST_SPLIT_SIZE = 0.1\\n\",\n    \"VALIDATION_SPLIT_SIZE = 0.1 # Relative to the remaining data after test split\\n\",\n    \"N_TRIALS_OPTUNA = 10 # Number of hyperparameter tuning trials\\n\",\n    \"\\n\",\n    \"Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\\n\",\n    \"Path(LOGGING_DIR).mkdir(parents=True, exist_ok=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Placeholder Data Loading --- \\n\",\n    \"# In a real project, this function would load data from files (CSV, JSON, etc.)\\n\",\n    \"# It should return a pandas DataFrame with at least 'transcript' and 'summary' columns.\\n\",\n    \"def load_data(use_dummy=True, dummy_size=50):\\n\",\n    \"    if use_dummy:\\n\",\n    \"        print(f\\\"Using dummy data with size {dummy_size}...\\\")\\n\",\n    \"        data = {\\n\",\n    \"            'transcript': [\\n\",\n    \"                f\\\"Speaker A: Hello team, let's discuss the Q3 project goals. Speaker B: Agreed. I think we should focus on user acquisition. Speaker A: Good point. Action item for John: Research acquisition channels by Friday. Speaker C: What about retention? Speaker A: Also important. Decision: We will allocate 60% resources to acquisition, 40% to retention. Any questions? Speaker B: No, sounds good. Vibe seems positive today.\\\" for i in range(dummy_size)\\n\",\n    \"            ],\\n\",\n    \"            'summary': [\\n\",\n    \"                f\\\"Meeting Summary {i}: Key Decision: Allocate 60% resources to acquisition, 40% to retention. Action Item: John to research acquisition channels by Friday. Sentiment: Positive.\\\" for i in range(dummy_size)\\n\",\n    \"            ]\\n\",\n    \"        }\\n\",\n    \"        df = pd.DataFrame(data)\\n\",\n    \"    else:\\n\",\n    \"        # Replace with actual data loading logic\\n\",\n    \"        # Example: df = pd.read_csv('path/to/your/transcripts.csv')\\n\",\n    \"        # Ensure columns are named 'transcript' and 'summary'\\n\",\n    \"        print(\\\"Loading real data (replace with actual implementation)...\\\")\\n\",\n    \"        # Dummy implementation for real data loading placeholder\\n\",\n    \"        data = {\\n\",\n    \"            'transcript': [\\\"Real transcript data point 1...\\\", \\\"Real transcript data point 2...\\\"],\\n\",\n    \"            'summary': [\\\"Real summary 1...\\\", \\\"Real summary 2...\\\"]\\n\",\n    \"        }\\n\",\n    \"        df = pd.DataFrame(data)\\n\",\n    \"        if 'transcript' not in df.columns or 'summary' not in df.columns:\\n\",\n    \"            raise ValueError(\\\"Dataframe must contain 'transcript' and 'summary' columns\\\")\\n\",\n    \"    return df\\n\",\n    \"\\n\",\n    \"# Load the data\\n\",\n    \"raw_df = load_data(use_dummy=USE_DUMMY_DATA, dummy_size=DUMMY_DATA_SIZE)\\n\",\n    \"print(f\\\"Loaded data shape: {raw_df.shape}\\\")\\n\",\n    \"print(raw_df.head())\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Data Splitting ---\\n\",\n    \"train_val_df, test_df = train_test_split(raw_df, test_size=TEST_SPLIT_SIZE, random_state=SEED)\\n\",\n    \"train_df, val_df = train_test_split(train_val_df, test_size=VALIDATION_SPLIT_SIZE / (1 - TEST_SPLIT_SIZE), random_state=SEED)\\n\",\n    \"\\n\",\n    \"print(f\\\"Train size: {len(train_df)}\\\")\\n\",\n    \"print(f\\\"Validation size: {len(val_df)}\\\")\\n\",\n    \"print(f\\\"Test size: {len(test_df)}\\\")\\n\",\n    \"\\n\",\n    \"# Convert pandas DataFrames to Hugging Face Datasets\\n\",\n    \"train_dataset = Dataset.from_pandas(train_df)\\n\",\n    \"val_dataset = Dataset.from_pandas(val_df)\\n\",\n    \"test_dataset = Dataset.from_pandas(test_df)\\n\",\n    \"\\n\",\n    \"raw_datasets = DatasetDict({\\n\",\n    \"    'train': train_dataset,\\n\",\n    \"    'validation': val_dataset,\\n\",\n    \"    'test': test_dataset\\n\",\n    \"})\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Preprocessing & Tokenization ---\\n\",\n    \"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\\n\",\n    \"\\n\",\n    \"def preprocess_function(examples):\\n\",\n    \"    inputs = [doc for doc in examples[\\\"transcript\\\"]]\\n\",\n    \"    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\\\"max_length\\\")\\n\",\n    \"\\n\",\n    \"    # Setup the tokenizer for targets\\n\",\n    \"    with tokenizer.as_target_tokenizer():\\n\",\n    \"        labels = tokenizer(examples[\\\"summary\\\"], max_length=MAX_TARGET_LENGTH, truncation=True, padding=\\\"max_length\\\")\\n\",\n    \"\\n\",\n    \"    model_inputs[\\\"labels\\\"] = labels[\\\"input_ids\\\"]\\n\",\n    \"    # Replace tokenizer.pad_token_id in the labels by -100 to ignore padding in the loss\\n\",\n    \"    model_inputs[\\\"labels\\\"] = [\\n\",\n    \"        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in model_inputs[\\\"labels\\\"]\\n\",\n    \"    ]\\n\",\n    \"    return model_inputs\\n\",\n    \"\\n\",\n    \"tokenized_datasets = raw_datasets.map(preprocess_function, batched=True, remove_columns=raw_datasets[\\\"train\\\"].column_names)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Model Definition (Summarization) ---\\n\",\n    \"model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\\n\",\n    \"\\n\",\n    \"# Data Collator\\n\",\n    \"data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Evaluation Metrics ---\\n\",\n    \"rouge_metric = load_metric(\\\"rouge\\\")\\n\",\n    \"\\n\",\n    \"def compute_metrics(eval_pred):\\n\",\n    \"    predictions, labels = eval_pred\\n\",\n    \"    # Decode generated summaries, replacing -100 in the labels as we can't decode them.\\n\",\n    \"    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\\n\",\n    \"    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\\n\",\n    \"    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\\n\",\n    \"    \\n\",\n    \"    # Rouge expects a newline after each sentence \\n\",\n    \"    decoded_preds = [\\\"\\\\n\\\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\\n\",\n    \"    decoded_labels = [\\\"\\\\n\\\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\\n\",\n    \"    \\n\",\n    \"    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\\n\",\n    \"    # Extract a few results\\n\",\n    \"    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\\n\",\n    \"    \\n\",\n    \"    # Add mean generated length\\n\",\n    \"    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\\n\",\n    \"    result[\\\"gen_len\\\"] = np.mean(prediction_lens)\\n\",\n    \"    \\n\",\n    \"    return {k: round(v, 4) for k, v in result.items()}\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Training Arguments (Initial) ---\\n\",\n    \"training_args = Seq2SeqTrainingArguments(\\n\",\n    \"    output_dir=OUTPUT_DIR,\\n\",\n    \"    evaluation_strategy=\\\"epoch\\\",\\n\",\n    \"    learning_rate=LEARNING_RATE,\\n\",\n    \"    per_device_train_batch_size=TRAIN_BATCH_SIZE,\\n\",\n    \"    per_device_eval_batch_size=EVAL_BATCH_SIZE,\\n\",\n    \"    weight_decay=WEIGHT_DECAY,\\n\",\n    \"    save_total_limit=3,\\n\",\n    \"    num_train_epochs=NUM_TRAIN_EPOCHS,\\n\",\n    \"    predict_with_generate=True,\\n\",\n    \"    fp16=torch.cuda.is_available(), # Enable mixed precision if GPU is available\\n\",\n    \"    logging_dir=LOGGING_DIR,\\n\",\n    \"    logging_steps=10,\\n\",\n    \"    save_strategy=\\\"epoch\\\",\\n\",\n    \"    load_best_model_at_end=True,\\n\",\n    \"    metric_for_best_model=\\\"rouge2\\\", # Choose metric to optimize for\\n\",\n    \"    greater_is_better=True,\\n\",\n    \"    report_to=\\\"tensorboard\\\", # Can integrate wandb or others\\n\",\n    \"    seed=SEED,\\n\",\n    \"    generation_max_length=MAX_TARGET_LENGTH,\\n\",\n    \"    generation_num_beams=4 # Add beam search for better generation during evaluation\\n\",\n    \")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Hyperparameter Tuning (Optuna) ---\\n\",\n    \"\\n\",\n    \"def model_init(trial):\\n\",\n    \"    # Reload the base model for each trial to avoid weight leakage\\n\",\n    \"    return AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\\n\",\n\n    \"def optuna_hp_space(trial):\\n\",\n    \"    return {\\n\",\n    \"        \\\"learning_rate\\\": trial.suggest_float(\\\"learning_rate\\\", 1e-6, 1e-4, log=True),\\n\",\n    \"        \\\"num_train_epochs\\\": trial.suggest_int(\\\"num_train_epochs\\\", 1, 5),\\n\",\n    \"        \\\"per_device_train_batch_size\\\": trial.suggest_categorical(\\\"per_device_train_batch_size\\\", [4, 8]),\\n\",\n    \"        \\\"weight_decay\\\": trial.suggest_float(\\\"weight_decay\\\", 0.0, 0.1),\\n\",\n    \"        \\\"generation_num_beams\\\": trial.suggest_int(\\\"generation_num_beams\\\", 2, 6),\\n\",\n    \"    }\\n\",\n    \"\\n\",\n    \"# Need to adjust batch size based on Optuna suggestion\\n\",\n    \"# The trainer needs to be re-initialized inside the objective function if batch size changes\\n\",\n    \"# However, the default hyperparameter_search handles this if we pass model_init\\n\",\n    \"\\n\",\n    \"trainer_for_tuning = Seq2SeqTrainer(\\n\",\n    \"    model=None, # Model will be initialized by model_init\\n\",\n    \"    args=training_args, # Use base training args, Optuna will override some\\n\",\n    \"    train_dataset=tokenized_datasets[\\\"train\\\"],\\n\",\n    \"    eval_dataset=tokenized_datasets[\\\"validation\\\"],\\n\",\n    \"    tokenizer=tokenizer,\\n\",\n    \"    data_collator=data_collator,\\n\",\n    \"    compute_metrics=compute_metrics,\\n\",\n    \"    model_init=model_init,\\n\",\n\n    \")\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\n--- Starting Hyperparameter Search ---\\\")\\n\",\n    \"best_run = trainer_for_tuning.hyperparameter_search(\\n\",\n    \"    direction=\\\"maximize\\\",\\n\",\n    \"    backend=\\\"optuna\\\",\\n\",\n    \"    hp_space=optuna_hp_space,\\n\",\n    \"    n_trials=N_TRIALS_OPTUNA,\\n\",\n    \"    compute_objective=lambda metrics: metrics[\\\"eval_rouge2\\\"], # Objective to maximize\\n\",\n    \"    # You might need to adjust resources (n_jobs) depending on your setup\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"print(\\\"--- Hyperparameter Search Finished ---\\\")\\n\",\n    \"print(f\\\"Best run details: {best_run}\\\")\\n\",\n    \"\\n\",\n    \"# Extract best hyperparameters\\n\",\n    \"best_hyperparameters = best_run.hyperparameters\\n\",\n    \"print(\\\"\\\\nBest Hyperparameters Found:\\\")\\n\",\n    \"print(json.dumps(best_hyperparameters, indent=2))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Train Final Model with Best Hyperparameters ---\\n\",\n    \"print(\\\"\\\\n--- Training Final Model with Best Hyperparameters ---\\\")\\n\",\n    \"\\n\",\n    \"# Update training arguments with best hyperparameters\\n\",\n    \"for param, value in best_hyperparameters.items():\\n\",\n    \"    setattr(training_args, param, value)\\n\",\n    \"\\n\",\n    \"# Ensure output_dir is set correctly after potential modification by Optuna\\n\",\n    \"training_args.output_dir = OUTPUT_DIR \\n\",\n    \"training_args.logging_dir = LOGGING_DIR\\n\",\n    \"\\n\",\n    \"# Re-initialize model and trainer with best hyperparameters\\n\",\n    \"final_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\\n\",\n    \"\\n\",\n    \"final_trainer = Seq2SeqTrainer(\\n\",\n    \"    model=final_model,\\n\",\n    \"    args=training_args,\\n\",\n    \"    train_dataset=tokenized_datasets[\\\"train\\\"],\\n\",\n    \"    eval_dataset=tokenized_datasets[\\\"validation\\\"],\\n\",\n    \"    tokenizer=tokenizer,\\n\",\n    \"    data_collator=data_collator,\\n\",\n    \"    compute_metrics=compute_metrics,\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"# Train the model\\n\",\n    \"train_result = final_trainer.train()\\n\",\n    \"\\n\",\n    \"print(\\\"--- Final Model Training Finished ---\\\")\\n\",\n    \"\\n\",\n    \"# Save training metrics\\n\",\n    \"metrics = train_result.metrics\\n\",\n    \"final_trainer.log_metrics(\\\"train\\\", metrics)\\n\",\n    \"final_trainer.save_metrics(\\\"train\\\", metrics)\\n\",\n    \"\\n\",\n    \"# Save the final model and tokenizer\\n\",\n    \"final_trainer.save_model(OUTPUT_DIR)\\n\",\n    \"tokenizer.save_pretrained(OUTPUT_DIR)\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nFinal model saved to {OUTPUT_DIR}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Evaluate Final Model on Test Set ---\\n\",\n    \"print(\\\"\\\\n--- Evaluating Final Model on Test Set ---\\\")\\n\",\n    \"\\n\",\n    \"test_results = final_trainer.evaluate(eval_dataset=tokenized_datasets[\\\"test\\\"])\\n\",\n    \"\\n\",\n    \"final_trainer.log_metrics(\\\"test\\\", test_results)\\n\",\n    \"final_trainer.save_metrics(\\\"test\\\", test_results)\\n\",\n    \"\\n\",\n    \"print(\\\"Test Set Evaluation Results:\\\")\\n\",\n    \"print(json.dumps(test_results, indent=2))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Demonstrate NER and Sentiment Analysis Integration ---\\n\",\n    \"print(\\\"\\\\n--- Demonstrating NER and Sentiment Analysis ---\\\")\\n\",\n    \"\\n\",\n    \"# Load the fine-tuned summarization model (if not already loaded)\\n\",\n    \"summarizer = pipeline(\\\"summarization\\\", model=OUTPUT_DIR, tokenizer=OUTPUT_DIR, device=0 if torch.cuda.is_available() else -1)\\n\",\n    \"\\n\",\n    \"# Load a sentiment analysis pipeline\\n\",\n    \"sentiment_analyzer = pipeline(\\\"sentiment-analysis\\\", device=0 if torch.cuda.is_available() else -1)\\n\",\n    \"\\n\",\n    \"# Example transcript for demonstration\\n\",\n    \"sample_transcript = \\\"\\\"\\\"\\n\",\n    \"Alice: Okay team, project Phoenix deadline is approaching. We need to finalize the deployment plan.\\n\",\n    \"Bob: I agree. I've drafted the initial steps. We need DevOps to review the server configurations by Wednesday.\\n\",\n    \"Charlie: I can coordinate with DevOps. Action Item for Charlie: Follow up with DevOps on server configs.\\n\",\n    \"Alice: Great. Also, Marketing needs the final feature list. Decision: Feature set X and Y are confirmed for launch.\\n\",\n    \"Bob: Sounds good. I'm a bit concerned about the integration testing timeline though.\\n\",\n    \"Alice: Let's schedule a separate meeting for that. For now, let's stick to the deployment plan. Vibe check: Seems focused but slightly stressed.\\n\",\n    \"Charlie: Agreed on the vibe. But we can manage.\\n\",\n    \"\\\"\\\"\\\"\\n\",\n    \"\\n\",\n    \"# 1. Generate Summary\\n\",\n    \"summary = summarizer(sample_transcript, max_length=MAX_TARGET_LENGTH, min_length=30, do_sample=False)[0]['summary_text']\\n\",\n    \"print(f\\\"\\\\nGenerated Summary:\\\\n{summary}\\\")\\n\",\n    \"\\n\",\n    \"# 2. Perform NER using spaCy\\n\",\n    \"doc = nlp_spacy(sample_transcript)\\n\",\n    \"print(\\\"\\\\nNamed Entities:\\\")\\n\",\n    \"entities = []\\n\",\n    \"for ent in doc.ents:\\n\",\n    \"    if ent.label_ in [\\\"PERSON\\\", \\\"ORG\\\", \\\"DATE\\\", \\\"GPE\\\"]: # Filter relevant entities\\n\",\n    \"        entities.append((ent.text, ent.label_))\\n\",\n    \"        print(f\\\"- {ent.text} ({ent.label_})\\\")\\n\",\n    \"\\n\",\n    \"# Simple Action Item Extraction (Rule-based example)\\n\",\n    \"action_items = []\\n\",\n    \"for sent in doc.sents:\\n\",\n    \"    if \\\"action item\\\" in sent.text.lower():\\n\",\n    \"        action_items.append(sent.text.strip())\\n\",\n    \"print(\\\"\\\\nPotential Action Items:\\\")\\n\",\n    \"for item in action_items:\\n\",\n    \"    print(f\\\"- {item}\\\")\\n\",\n    \"\\n\",\n    \"# 3. Perform Sentiment Analysis (Overall)\\n\",\n    \"# For time segments, you'd split the transcript and run sentiment on each part\\n\",\n    \"overall_sentiment = sentiment_analyzer(sample_transcript[:1024]) # Limit input size for standard models\\n\",\n    \"print(f\\\"\\\\nOverall Sentiment:\\\\n{overall_sentiment}\\\")\\n\",\n    \"\\n\",\n    \"# Example: Sentiment over time (simple split)\\n\",\n    \"print(\\\"\\\\nSentiment 'Vibe Check' (Segmented):\\\")\\n\",\n    \"sentences = nltk.sent_tokenize(sample_transcript)\\n\",\n    \"num_segments = 3\\n\",\n    \"segment_len = len(sentences) // num_segments\\n\",\n    \"for i in range(num_segments):\\n\",\n    \"    start = i * segment_len\\n\",\n    \"    end = (i + 1) * segment_len if i < num_segments - 1 else len(sentences)\\n\",\n    \"    segment_text = \\\" \\\".join(sentences[start:end])\\n\",\n    \"    if segment_text:\\n\",\n    \"        segment_sentiment = sentiment_analyzer(segment_text[:1024])\\n\",\n    \"        print(f\\\"- Segment {i+1}: {segment_sentiment}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Results Visualization ---\\n\",\n    \"print(\\\"\\\\n--- Visualizing Results ---\\\")\\n\",\n    \"\\n\",\n    \"# Plot training history (Loss)\\n\",\n    \"try:\\n\",\n    \"    log_history = final_trainer.state.log_history\\n\",\n    \"    \\n\",\n    \"    train_logs = [log for log in log_history if 'loss' in log]\\n\",\n    \"    eval_logs = [log for log in log_history if 'eval_loss' in log]\\n\",\n    \"\\n\",\n    \"    train_steps = [log['step'] for log in train_logs]\\n\",\n    \"    train_loss = [log['loss'] for log in train_logs]\\n\",\n    \"    \\n\",\n    \"    eval_steps = [log['step'] for log in eval_logs]\\n\",\n    \"    eval_loss = [log['eval_loss'] for log in eval_logs]\\n\",\n    \"    eval_rouge2 = [log['eval_rouge2'] for log in eval_logs]\\n\",\n    \"\\n\",\n    \"    plt.figure(figsize=(12, 6))\\n\",\n    \"    sns.set_style(\\\"whitegrid\\\")\\n\",\n    \"\\n\",\n    \"    plt.subplot(1, 2, 1)\\n\",\n    \"    plt.plot(train_steps, train_loss, label='Training Loss')\\n\",\n    \"    plt.plot(eval_steps, eval_loss, label='Validation Loss', marker='o')\\n\",\n    \"    plt.title('Training and Validation Loss')\\n\",\n    \"    plt.xlabel('Steps')\\n\",\n    \"    plt.ylabel('Loss')\\n\",\n    \"    plt.legend()\\n\",\n    \"\\n\",\n    \"    plt.subplot(1, 2, 2)\\n\",\n    \"    plt.plot(eval_steps, eval_rouge2, label='Validation ROUGE-2', marker='o', color='green')\\n\",\n    \"    plt.title('Validation ROUGE-2 Score')\\n\",\n    \"    plt.xlabel('Steps')\\n\",\n    \"    plt.ylabel('ROUGE-2 F1 Score')\\n\",\n    \"    plt.legend()\\n\",\n    \"\\n\",\n    \"    plt.tight_layout()\\n\",\n    \"    plt.savefig(os.path.join(OUTPUT_DIR, \\\"training_plots.png\\\"))\\n\",\n    \"    plt.show()\\n\",\n    \"    print(f\\\"Training plots saved to {os.path.join(OUTPUT_DIR, 'training_plots.png')}\\\")\\n\",\n    \"\\n\",\n    \"except Exception as e:\\n\",\n    \"    print(f\\\"Could not plot training history: {e}\\\")\\n\","
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}