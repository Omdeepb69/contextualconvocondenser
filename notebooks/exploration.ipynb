{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "```json\n{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# ContextualConvoCondenser: Data Exploration\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 1. Setup and Data Loading\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import pandas as pd\\n\",\n    \"import numpy as np\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"import seaborn as sns\\n\",\n    \"import nltk\\n\",\n    \"from nltk.sentiment.vader import SentimentIntensityAnalyzer\\n\",\n    \"from nltk.tokenize import word_tokenize, sent_tokenize\\n\",\n    \"from nltk.probability import FreqDist\\n\",\n    \"from wordcloud import WordCloud\\n\",\n    \"import spacy\\n\",\n    \"from collections import Counter\\n\",\n    \"import re\\n\",\n    \"import warnings\\n\",\n    \"\\n\",\n    \"# Suppress specific warnings for cleaner output\\n\",\n    \"warnings.filterwarnings('ignore', category=FutureWarning)\\n\",\n    \"warnings.filterwarnings('ignore', category=UserWarning)\\n\",\n    \"\\n\",\n    \"# Download necessary NLTK data (if not already downloaded)\\n\",\n    \"try:\\n\",\n    \"    nltk.data.find('sentiment/vader_lexicon.zip')\\n\",\n    \"except nltk.downloader.DownloadError:\\n\",\n    \"    nltk.download('vader_lexicon')\\n\",\n    \"try:\\n\",\n    \"    nltk.data.find('tokenizers/punkt')\\n\",\n    \"except nltk.downloader.DownloadError:\\n\",\n    \"    nltk.download('punkt')\\n\",\n    \"\\n\",\n    \"# Load spaCy model (download if needed)\\n\",\n    \"try:\\n\",\n    \"    nlp = spacy.load('en_core_web_sm')\\n\",\n    \"except OSError:\\n\",\n    \"    print('Downloading spaCy en_core_web_sm model...')\\n\",\n    \"    spacy.cli.download('en_core_web_sm')\\n\",\n    \"    nlp = spacy.load('en_core_web_sm')\\n\",\n    \"\\n\",\n    \"# Set plot style\\n\",\n    \"sns.set_style('whitegrid')\\n\",\n    \"plt.rcParams['figure.figsize'] = (12, 6)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Create Synthetic Sample Data\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Create synthetic conversation data representing transcripts\\n\",\n    \"data = {\\n\",\n    \"    'conversation_id': ['conv_001'] * 8 + ['conv_002'] * 10 + ['conv_003'] * 7,\\n\",\n    \"    'timestamp': pd.to_datetime([\\n\",\n    \"        '2023-10-26 09:01:00', '2023-10-26 09:01:30', '2023-10-26 09:02:15', '2023-10-26 09:03:00',\\n\",\n    \"        '2023-10-26 09:03:45', '2023-10-26 09:04:30', '2023-10-26 09:05:00', '2023-10-26 09:05:45',\\n\",\n    \"        '2023-10-27 14:00:00', '2023-10-27 14:00:45', '2023-10-27 14:01:30', '2023-10-27 14:02:00',\\n\",\n    \"        '2023-10-27 14:02:45', '2023-10-27 14:03:30', '2023-10-27 14:04:15', '2023-10-27 14:05:00',\\n\",\n    \"        '2023-10-27 14:05:45', '2023-10-27 14:06:30',\\n\",\n    \"        '2023-10-28 11:30:00', '2023-10-28 11:30:40', '2023-10-28 11:31:15', '2023-10-28 11:32:05',\\n\",\n    \"        '2023-10-28 11:32:50', '2023-10-28 11:33:30', '2023-10-28 11:34:00'\\n\",\n    \"    ]),\\n\",\n    \"    'speaker': [\\n\",\n    \"        'Alice', 'Bob', 'Alice', 'Charlie', 'Bob', 'Alice', 'Charlie', 'Bob', # conv_001\\n\",\n    \"        'SupportAgent', 'Customer', 'SupportAgent', 'Customer', 'SupportAgent', 'Customer', 'SupportAgent', 'Customer', 'SupportAgent', 'Customer', # conv_002\\n\",\n    \"        'David', 'Eve', 'David', 'Eve', 'Frank', 'David', 'Eve' # conv_003\\n\",\n    \"    ],\\n\",\n    \"    'utterance': [\\n\",\n    \"        \\\"Okay team, let's kick off the project planning.\\\", \\\"Agreed. First, we need to define the key milestones.\\\", \\\"Right. Milestone 1 should be the requirements gathering, due next Friday.\\\", \\\"I can take the lead on that. I'll draft the document.\\\", \\\"Thanks, Charlie. Bob, can you handle the resource allocation plan?\\\", \\\"Sure, I'll get that done by Wednesday.\\\", \\\"Excellent. Any initial concerns?\\\", \\\"Just the tight deadline for Milestone 1, but it's doable.\\\", # conv_001\\n\",\n    \"        \\\"Hello, thank you for calling Tech Support. How can I help you?\\\", \\\"Hi, my internet connection keeps dropping.\\\", \\\"I understand that must be frustrating. Can you tell me when this started?\\\", \\\"It's been happening intermittently for the past two days.\\\", \\\"Okay, let's try resetting your modem. Could you unplug it for 30 seconds?\\\", \\\"Alright, doing that now... Okay, it's plugged back in.\\\", \\\"Great. Please give it a minute or two to reconnect. Is the 'Internet' light solid now?\\\", \\\"Yes, it looks stable for now.\\\", \\\"Good. If the issue persists, please call us back with reference number 12345. Is there anything else?\\\", \\\"No, that's all. Thank you!\\\", # conv_002\\n\",\n    \"        \\\"Morning everyone. Let's discuss the Q3 results.\\\", \\\"Morning. Overall, sales are up 15% year-over-year, which is fantastic.\\\", \\\"Great news! Which product line drove most of that growth?\\\", \\\"The new 'Gadget Pro' accounted for nearly 60% of the increase.\\\", \\\"Impressive. Frank, any updates from marketing on the Gadget Pro campaign?\\\", \\\"Yes, the digital campaign exceeded targets. We need to decide on the budget for Q4 continuation.\\\", \\\"Okay, let's schedule a follow-up on that. Eve, please set up a meeting for next Tuesday.\\\" # conv_003\\n\",\n    \"    ]\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"df = pd.DataFrame(data)\\n\",\n    \"\\n\",\n    \"# Combine utterances into full transcripts for each conversation\\n\",\n    \"transcripts = df.groupby('conversation_id').agg(\\n\",\n    \"    full_transcript=('utterance', lambda x: ' '.join(x)),\\n\",\n    \"    start_time=('timestamp', 'min'),\\n\",\n    \"    end_time=('timestamp', 'max'),\\n\",\n    \"    num_utterances=('utterance', 'count'),\\n\",\n    \"    speakers=('speaker', lambda x: list(x.unique()))\\n\",\n    \").reset_index()\\n\",\n    \"\\n\",\n    \"transcripts['duration'] = transcripts['end_time'] - transcripts['start_time']\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Data Inspection\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"print(\\\"Utterance Data Head:\\\")\\n\",\n    \"print(df.head())\\n\",\n    \"print(\\\"\\\\nUtterance Data Info:\\\")\\n\",\n    \"df.info()\\n\",\n    \"print(\\\"\\\\nUtterance Data Missing Values:\\\")\\n\",\n    \"print(df.isnull().sum())\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"print(\\\"\\\\nTranscript Data Head:\\\")\\n\",\n    \"print(transcripts.head())\\n\",\n    \"print(\\\"\\\\nTranscript Data Info:\\\")\\n\",\n    \"transcripts.info()\\n\",\n    \"print(\\\"\\\\nTranscript Data Missing Values:\\\")\\n\",\n    \"print(transcripts.isnull().sum())\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 2. Exploratory Data Analysis (EDA)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Distribution of Utterance Length\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"df['utterance_length'] = df['utterance'].apply(lambda x: len(word_tokenize(x)))\\n\",\n    \"\\n\",\n    \"plt.figure(figsize=(10, 5))\\n\",\n    \"sns.histplot(df['utterance_length'], bins=15, kde=True)\\n\",\n    \"plt.title('Distribution of Utterance Length (Number of Words)')\\n\",\n    \"plt.xlabel('Number of Words')\\n\",\n    \"plt.ylabel('Frequency')\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Distribution of Transcript Length\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"transcripts['transcript_length'] = transcripts['full_transcript'].apply(lambda x: len(word_tokenize(x)))\\n\",\n    \"\\n\",\n    \"plt.figure(figsize=(10, 5))\\n\",\n    \"sns.histplot(transcripts['transcript_length'], bins=5, kde=False)\\n\",\n    \"plt.title('Distribution of Full Transcript Length (Number of Words)')\\n\",\n    \"plt.xlabel('Number of Words')\\n\",\n    \"plt.ylabel('Frequency')\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Speaker Contribution\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"plt.figure(figsize=(12, 6))\\n\",\n    \"sns.countplot(data=df, y='speaker', order=df['speaker'].value_counts().index, palette='viridis')\\n\",\n    \"plt.title('Number of Utterances per Speaker')\\n\",\n    \"plt.xlabel('Number of Utterances')\\n\",\n    \"plt.ylabel('Speaker')\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Word Cloud of All Transcripts\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"all_text = ' '.join(transcripts['full_transcript'])\\n\",\n    \"wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\\n\",\n    \"\\n\",\n    \"plt.figure(figsize=(15, 7))\\n\",\n    \"plt.imshow(wordcloud, interpolation='bilinear')\\n\",\n    \"plt.axis('off')\\n\",\n    \"plt.title('Word Cloud for All Transcripts')\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Conversation Duration Distribution\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Convert duration to total seconds for plotting\\n\",\n    \"transcripts['duration_seconds'] = transcripts['duration'].dt.total_seconds()\\n\",\n    \"\\n\",\n    \"plt.figure(figsize=(10, 5))\\n\",\n    \"sns.histplot(transcripts['duration_seconds'], bins=5, kde=False)\\n\",\n    \"plt.title('Distribution of Conversation Duration')\\n\",\n    \"plt.xlabel('Duration (seconds)')\\n\",\n    \"plt.ylabel('Frequency')\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 3. Statistical Analysis\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Basic Text Statistics\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Average utterance length per conversation\\n\",\n    \"avg_utterance_len = df.groupby('conversation_id')['utterance_length'].mean()\\n\",\n    \"transcripts = transcripts.merge(avg_utterance_len.rename('avg_utterance_len'), on='conversation_id')\\n\",\n    \"\\n\",\n    \"# Vocabulary richness (Type-Token Ratio) per conversation\\n\",\n    \"def calculate_ttr(text):\\n\",\n    \"    tokens = [token.lower() for token in word_tokenize(text) if token.isalpha()]\\n\",\n    \"    if not tokens:\\n\",\n    \"        return 0\\n\",\n    \"    return len(set(tokens)) / len(tokens)\\n\",\n    \"\\n\",\n    \"transcripts['ttr'] = transcripts['full_transcript'].apply(calculate_ttr)\\n\",\n    \"\\n\",\n    \"print(\\\"Transcript Statistics:\\\")\\n\",\n    \"print(transcripts[['conversation_id', 'num_utterances', 'transcript_length', 'avg_utterance_len', 'ttr', 'duration_seconds']].describe())\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Most Common Words (excluding stopwords)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from nltk.corpus import stopwords\\n\",\n    \"\\n\",\n    \"# Download stopwords if necessary\\n\",\n    \"try:\\n\",\n    \"    nltk.data.find('corpora/stopwords')\\n\",\n    \"except nltk.downloader.DownloadError:\\n\",\n    \"    nltk.download('stopwords')\\n\",\n    \"\\n\",\n    \"stop_words = set(stopwords.words('english'))\\n\",\n    \"\\n\",\n    \"all_tokens = word_tokenize(all_text.lower())\\n\",\n    \"filtered_tokens = [word for word in all_tokens if word.isalpha() and word not in stop_words]\\n\",\n    \"\\n\",\n    \"fdist = FreqDist(filtered_tokens)\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nTop 20 Most Common Words (excluding stopwords):\\\")\\n\",\n    \"print(fdist.most_common(20))\\n\",\n    \"\\n\",\n    \"# Plot frequency distribution\\n\",\n    \"plt.figure(figsize=(12, 6))\\n\",\n    \"fdist.plot(20, cumulative=False)\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 4. Feature Engineering Experiments\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Sentiment Analysis per Utterance\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"sid = SentimentIntensityAnalyzer()\\n\",\n    \"\\n\",\n    \"df['sentiment_scores'] = df['utterance'].apply(lambda x: sid.polarity_scores(x))\\n\",\n    \"df['sentiment_compound'] = df['sentiment_scores'].apply(lambda x: x['compound'])\\n\",\n    \"df['sentiment_label'] = df['sentiment_compound'].apply(lambda c: 'positive' if c >= 0.05 else ('negative' if c <= -0.05 else 'neutral'))\\n\",\n    \"\\n\",\n    \"print(\\\"Sentiment Analysis per Utterance (Sample):\\\")\\n\",\n    \"print(df[['utterance', 'sentiment_compound', 'sentiment_label']].head())\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Visualize sentiment distribution\\n\",\n    \"plt.figure(figsize=(8, 5))\\n\",\n    \"sns.countplot(data=df, x='sentiment_label', palette={'positive':'green', 'neutral':'grey', 'negative':'red'})\\n\",\n    \"plt.title('Distribution of Utterance Sentiment')\\n\",\n    \"plt.xlabel('Sentiment Label')\\n\",\n    \"plt.ylabel('Count')\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Sentiment Trend within Conversations ('Vibe Check')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"plt.figure(figsize=(15, 7))\\n\",\n    \"for conv_id in df['conversation_id'].unique():\\n\",\n    \"    conv_df = df[df['conversation_id'] == conv_id].reset_index()\\n\",\n    \"    plt.plot(conv_df.index, conv_df['sentiment_compound'], marker='o', linestyle='-', label=conv_id)\\n\",\n    \"\\n\",\n    \"plt.title('Sentiment Trend Over Utterances within Conversations')\\n\",\n    \"plt.xlabel('Utterance Sequence')\\n\",\n    \"plt.ylabel('Sentiment Compound Score')\\n\",\n    \"plt.legend(title='Conversation ID')\\n\",\n    \"plt.axhline(0.05, color='green', linestyle='--', alpha=0.5, label='Positive Threshold')\\n\",\n    \"plt.axhline(-0.05, color='red', linestyle='--', alpha=0.5, label='Negative Threshold')\\n\",\n    \"plt.ylim(-1.1, 1.1)\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Named Entity Recognition (NER) - Example\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def extract_entities(text):\\n\",\n    \"    doc = nlp(text)\\n\",\n    \"    entities = [(ent.text, ent.label_) for ent in doc.ents]\\n\",\n    \"    return entities\\n\",\n    \"\\n\",\n    \"# Apply NER to a sample transcript\\n\",\n    \"sample_transcript = transcripts.loc[0, 'full_transcript']\\n\",\n    \"sample_entities = extract_entities(sample_transcript)\\n\",\n    \"\\n\",\n    \"print(f\\\"Entities for Conversation {transcripts.loc[0, 'conversation_id']}:\\\")\\n\",\n    \"print(sample_entities)\\n\",\n    \"\\n\",\n    \"# Count entity types across all transcripts\\n\",\n    \"all_entities = []\\n\",\n    \"for transcript in transcripts['full_transcript']:\\n\",\n    \"    all_entities.extend(extract_entities(transcript))\\n\",\n    \"\\n\",\n    \"entity_labels = [label for text, label in all_entities]\\n\",\n    \"entity_counts = Counter(entity_labels)\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nOverall Entity Type Counts:\\\")\\n\",\n    \"print(entity_counts)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Visualize top N entity types\\n\",\n    \"common_entities = entity_counts.most_common(10)\\n\",\n    \"labels, values = zip(*common_entities)\\n\",\n    \"\\n\",\n    \"plt.figure(figsize=(12, 6))\\n\",\n    \"sns.barplot(x=list(values), y=list(labels), palette='mako')\\n\",\n    \"plt.title('Top 10 Most Common Entity Types Across All Transcripts')\\n\",\n    \"plt.xlabel('Frequency')\\n\",\n    \"plt.ylabel('Entity Type')\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Keyword Spotting (Action Items / Decisions)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Simple keyword spotting for potential action items or decisions\\n\",\n    \"action_keywords = ['i will', 'i\\'ll', 'can you', 'assign', 'task', 'action item', 'due', 'deadline', 'follow-up', 'schedule']\\n\",\n    \"decision_keywords = ['agreed', 'decide', 'decision', 'plan is', 'we need to', 'confirm', 'okay', 'right']\\n\",\n    \"\\n\",\n    \"def count_keywords(text, keywords):\\n\",\n    \"    count = 0\\n\",\n    \"    text_lower = text.lower()\\n\",\n    \"    for keyword in keywords:\\n\",\n    \"        count += len(re.findall(r'\\\\b' + re.escape(keyword) + r'\\\\b', text_lower))\\n\",\n    \"    return count\\n\",\n    \"\\n\",\n    \"df['action_keyword_count'] = df['utterance'].apply(lambda x: count_keywords(x, action_keywords))\\n\",\n    \"df['decision_keyword_count'] = df['utterance'].apply(lambda x: count_keywords(x, decision_keywords))\\n\",\n    \"\\n\",\n    \"transcripts['total_action_keywords'] = df.groupby('conversation_id')['action_keyword_count'].sum()\\n\",\n    \"transcripts['total_decision_keywords'] = df.groupby('conversation_id')['decision_keyword_count'].sum()\\n\",\n    \"\\n\",\n    \"print(\\\"Keyword Counts per Transcript:\\\")\\n\",\n    \"print(transcripts[['conversation_id', 'total_action_keywords', 'total_decision_keywords']])\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 5. Initial Model Testing\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Basic Summarization with Hugging Face Transformers\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Note: This requires the `transformers` and `torch` (or `tensorflow`) libraries\\n\",\n    \"# pip install transformers torch\\n\",\n    \"from transformers import pipeline\\n\",\n    \"\\n\",\n    \"# Load a pre-trained summarization pipeline (using a small model for speed)\\n\",\n    \"# Using BART as an example, T5 is another good option.\\n\",\n    \"try:\\n\",\n    \"    summarizer = pipeline(\\\"summarization\\\", model=\\\"sshleifer/distilbart-cnn-6-6\\\", device=-1) # Use CPU\\n\",\n    \"    print(\\\"Summarization pipeline loaded successfully.\\\")\\n\",\n    \"except Exception as e:\\n\",\n    \"    print(f\\\"Error loading summarization pipeline: {e}\\\")\\n\",\n    \"    print(\\\"Skipping summarization test. Ensure 'transformers' and a backend (torch/tensorflow) are installed.\\\")\\n\",\n    \"    summarizer = None\\n\",\n    \"\\n\",\n    \"if summarizer:\\n\",\n    \"    # Select a sample transcript to summarize\\n\",\n    \"    sample_conv_id = 'conv_001'\\n\",\n    \"    text_to_summarize = transcripts[transcripts['conversation_id'] == sample_conv_id]['full_transcript'].iloc[0]\\n\",\n    \"\\n\",\n    \"    print(f\\\"\\\\nOriginal Transcript ({sample_conv_id}):\\\")\\n\",\n    \"    print(text_to_summarize)\\n\",\n    \"\\n\",\n    \"    # Generate summary (adjust max/min length as needed)\\n\",\n    \"    # Note: Summarization models have input length limits. Longer texts might need chunking.\\n\",\n    \"    try:\\n\",\n    \"        summary = summarizer(text_to_summarize, max_length=100, min_length=25, do_sample=False)[0]['summary_text']\\n\",\n    \"        print(\\\"\\\\nGenerated Summary:\\\")\\n\",\n    \"        print(summary)\\n\",\n    \"    except Exception as e:\\n\",\n    \"        print(f\\\"\\\\nError during summarization: {e}\\\")\\n\",\n    \"        # Common issue: Input text exceeds model's maximum sequence length.\\n\",\n    \"        if \\\"maximum sequence length\\\" in str(e):\\n\",\n    \"             print(\\\"Input text might be too long for this model. Consider chunking or using a model with a larger context window.\\\")\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Sentiment Analysis with Hugging Face Transformers (Alternative)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Using a transformer-based sentiment analysis pipeline\\n\",\n    \"try:\\n\",\n    \"    sentiment_pipeline = pipeline(\\\"sentiment-analysis\\\", model=\\\"distilbert-base-uncased-finetuned-sst-2-english\\\", device=-1) # Use CPU\\n\",\n    \"    print(\\\"\\\\nSentiment analysis pipeline loaded successfully.\\\")\\n\",\n    \"except Exception as e:\\n\",\n    \"    print(f\\\"\\\\nError loading sentiment analysis pipeline: {e}\\\")\\n\",\n    \"    print(\\\"Skipping transformer sentiment test. Ensure 'transformers' and a backend (torch/tensorflow) are installed.\\\")\\n\",\n    \"    sentiment_pipeline = None\\n\",\n    \"\\n\",\n    \"if sentiment_pipeline:\\n\",\n    \"    # Analyze sentiment of a few sample utterances\\n\",\n    \"    sample_utterances = df['utterance'].head(3).tolist()\\n\",\n    \"    print(\\\"\\\\nAnalyzing sentiment of sample utterances:\\\")\\n\",\n    \"    for utt in sample_utterances:\\n\",\n    \"        try:\\n\",\n    \"            result = sentiment_pipeline(utt)[0]\\n\",\n    \"            print(f\\\"- Utterance: '{utt}'\\\")\\n\",\n    \"            print(f\\\"  Sentiment: {result['label']}, Score: {result['score']:.4f}\\\")\\n\",\n    \"        except Exception as e:\\n\",\n    \"            print(f\\\"\\\\nError during sentiment analysis for utterance '{utt}': {e}\\\")\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## End of Exploration\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}